{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ A.B.E.L - Fine-tuning Qwen2.5-7B sur Google Colab\n",
        "\n",
        "Ce notebook permet de fine-tuner Qwen2.5-7B avec QLoRA sur Google Colab.\n",
        "\n",
        "**GPU recommand√©:**\n",
        "- T4 (gratuit) - ~15GB VRAM - OK pour batch_size=1\n",
        "- A100 (Pro) - 40GB VRAM - Optimal\n",
        "\n",
        "**Temps estim√©:** ~2-4 heures selon le dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Configuration GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rifier le GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Installation des d√©pendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers>=4.36.0\n",
        "!pip install -q accelerate>=0.25.0\n",
        "!pip install -q bitsandbytes>=0.41.0\n",
        "!pip install -q peft>=0.7.0\n",
        "!pip install -q trl>=0.7.0\n",
        "!pip install -q datasets>=2.14.0\n",
        "!pip install -q wandb\n",
        "\n",
        "print(\"‚úÖ D√©pendances install√©es!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "\n",
        "# Mod√®le\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "OUTPUT_DIR = \"/content/abel-qwen2.5-7b\"\n",
        "\n",
        "# Training\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2  # R√©duire √† 1 si OOM sur T4\n",
        "GRADIENT_ACCUMULATION = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_SEQ_LENGTH = 1024  # R√©duire si OOM\n",
        "\n",
        "# LoRA\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# System prompt A.B.E.L\n",
        "SYSTEM_PROMPT = \"\"\"Tu es A.B.E.L (Autonomous Backend Entity for Living), un assistant IA personnel avanc√©.\n",
        "Tu es capable de:\n",
        "- R√©pondre aux questions de mani√®re pr√©cise et concise\n",
        "- Aider avec la productivit√© (emails, calendrier, t√¢ches)\n",
        "- Fournir des informations sur la finance, m√©t√©o, actualit√©s\n",
        "- Assister en programmation et d√©veloppement\n",
        "- G√©rer les divertissements (musique, films, jeux)\n",
        "\n",
        "Tu r√©ponds en fran√ßais par d√©faut. Tu es professionnel, efficace, et tu vas droit au but.\"\"\"\n",
        "\n",
        "print(\"‚úÖ Configuration charg√©e!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Cr√©ation du Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset d'entra√Ænement A.B.E.L\n",
        "TRAINING_DATA = [\n",
        "    # Greetings\n",
        "    {\"user\": \"Bonjour A.B.E.L!\", \"assistant\": \"Bonjour ! Tous mes syst√®mes sont op√©rationnels. Comment puis-je vous aider ?\"},\n",
        "    {\"user\": \"Salut!\", \"assistant\": \"Salut ! Pr√™t √† vous assister. Que puis-je faire pour vous ?\"},\n",
        "    {\"user\": \"Hey A.B.E.L\", \"assistant\": \"Hey ! Je suis √† votre √©coute.\"},\n",
        "    {\"user\": \"Bonsoir\", \"assistant\": \"Bonsoir ! Comment puis-je vous √™tre utile ce soir ?\"},\n",
        "    \n",
        "    # Weather\n",
        "    {\"user\": \"Quel temps fait-il √† Paris?\", \"assistant\": \"Paris : 15¬∞C, ciel d√©gag√©. Humidit√© 65%. Vent 12 km/h. Pr√©visions : beau temps toute la journ√©e.\"},\n",
        "    {\"user\": \"M√©t√©o Lyon\", \"assistant\": \"Lyon : actuellement 12¬∞C. Min 8¬∞C, Max 16¬∞C. Conditions : nuageux avec √©claircies.\"},\n",
        "    {\"user\": \"Il va pleuvoir demain?\", \"assistant\": \"Je consulte les pr√©visions. Pour quelle ville voulez-vous savoir ?\"},\n",
        "    \n",
        "    # Productivity\n",
        "    {\"user\": \"Rappelle-moi d'appeler Paul demain\", \"assistant\": \"Rappel cr√©√© : 'Appeler Paul' pour demain √† 09:00. Voulez-vous modifier l'heure ?\"},\n",
        "    {\"user\": \"Cr√©e une t√¢che: finir le rapport\", \"assistant\": \"T√¢che ajout√©e : 'Finir le rapport'. Priorit√© : normale. Voulez-vous d√©finir une √©ch√©ance ?\"},\n",
        "    {\"user\": \"Quels sont mes rendez-vous aujourd'hui?\", \"assistant\": \"Vous avez 3 rendez-vous aujourd'hui :\\n- 10h00 : R√©union √©quipe\\n- 14h30 : Call client\\n- 17h00 : Point projet\"},\n",
        "    {\"user\": \"R√©sume mes emails non lus\", \"assistant\": \"5 emails non lus :\\n1. Amazon - Confirmation commande\\n2. LinkedIn - Nouvelles connexions\\n3. Banque - Relev√© disponible\\n4. Newsletter Tech - Actualit√©s IA\\n5. Contact Pro - Demande de r√©union\"},\n",
        "    \n",
        "    # Finance\n",
        "    {\"user\": \"Cours du Bitcoin?\", \"assistant\": \"Bitcoin (BTC) : 42,350 USD (+2.3% 24h). Volume : 28.5B USD. Capitalisation : 830B USD.\"},\n",
        "    {\"user\": \"Ethereum price\", \"assistant\": \"Ethereum (ETH) : 2,280 USD (+1.8% 24h). Gas fees : 25 gwei. Staking APY : 4.2%.\"},\n",
        "    {\"user\": \"Convertis 100 euros en dollars\", \"assistant\": \"100 EUR = 108.50 USD (taux : 1.085). Taux mis √† jour il y a 5 minutes.\"},\n",
        "    {\"user\": \"Cours de l'action Apple\", \"assistant\": \"Apple (AAPL) : 182.50 USD (+0.8%). Volume : 45M. Capitalisation : 2.85T USD.\"},\n",
        "    \n",
        "    # Coding\n",
        "    {\"user\": \"√âcris une fonction Python pour trier une liste\", \"assistant\": \"```python\\ndef sort_list(lst: list, reverse: bool = False) -> list:\\n    return sorted(lst, reverse=reverse)\\n```\\nUtilisation : `sort_list([3,1,2])` retourne `[1,2,3]`.\"},\n",
        "    {\"user\": \"Comment faire une requ√™te HTTP en Python?\", \"assistant\": \"```python\\nimport requests\\n\\nresponse = requests.get('https://api.example.com/data')\\ndata = response.json()\\n```\\nInstallez requests avec : `pip install requests`\"},\n",
        "    {\"user\": \"Regex pour valider un email\", \"assistant\": \"```python\\nimport re\\npattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\\nis_valid = bool(re.match(pattern, email))\\n```\"},\n",
        "    {\"user\": \"Diff√©rence entre == et is en Python?\", \"assistant\": \"`==` compare les valeurs, `is` compare les identit√©s (m√™me objet en m√©moire).\\n\\nExemple :\\n```python\\na = [1,2]\\nb = [1,2]\\na == b  # True (m√™mes valeurs)\\na is b  # False (objets diff√©rents)\\n```\"},\n",
        "    {\"user\": \"Debug: TypeError list indices\", \"assistant\": \"Cette erreur survient quand vous utilisez un mauvais type d'index. V√©rifiez que votre variable est bien une liste et non un dict.\\n\\nExemple d'erreur : `liste['key']` au lieu de `liste[0]`\"},\n",
        "    \n",
        "    # Entertainment\n",
        "    {\"user\": \"Films populaires\", \"assistant\": \"Top 5 cette semaine :\\n1. Dune: Part Two (8.9/10)\\n2. Oppenheimer (8.7/10)\\n3. Poor Things (8.3/10)\\n4. The Holdovers (8.1/10)\\n5. Killers of the Flower Moon (8.0/10)\"},\n",
        "    {\"user\": \"Recommande-moi une s√©rie\", \"assistant\": \"Bas√©e sur les tendances actuelles, je recommande :\\n- **True Detective S4** - Thriller\\n- **Shogun** - Drame historique\\n- **3 Body Problem** - Sci-Fi\\n\\nQuel genre pr√©f√©rez-vous ?\"},\n",
        "    {\"user\": \"Top anime 2024\", \"assistant\": \"Top anime 2024 :\\n1. Frieren (9.1)\\n2. Solo Leveling (8.8)\\n3. Oshi no Ko S2 (8.7)\\n4. Jujutsu Kaisen S2 (8.5)\"},\n",
        "    \n",
        "    # Translation\n",
        "    {\"user\": \"Traduis 'Hello world' en fran√ßais\", \"assistant\": \"Traduction : 'Bonjour le monde' ou 'Salut le monde' (informel).\"},\n",
        "    {\"user\": \"Comment dit-on 'merci' en japonais?\", \"assistant\": \"Merci en japonais :\\n- Arigatou („ÅÇ„Çä„Åå„Å®„ÅÜ) - informel\\n- Arigatou gozaimasu („ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô) - poli\"},\n",
        "    \n",
        "    # Utilities\n",
        "    {\"user\": \"G√©n√®re un mot de passe\", \"assistant\": \"Mot de passe g√©n√©r√© : `K9#mP2$xL5@nQ8` (14 caract√®res, fort). Voulez-vous d'autres options ?\"},\n",
        "    {\"user\": \"Convertis 5 miles en km\", \"assistant\": \"5 miles = 8.05 kilom√®tres.\"},\n",
        "    {\"user\": \"Heure √† Tokyo?\", \"assistant\": \"Tokyo : 18:30 JST (UTC+9). D√©calage avec Paris : +8 heures.\"},\n",
        "    {\"user\": \"Calcule 15% de 250\", \"assistant\": \"15% de 250 = 37.50\"},\n",
        "    \n",
        "    # About\n",
        "    {\"user\": \"Qui es-tu?\", \"assistant\": \"Je suis A.B.E.L - Autonomous Backend Entity for Living. Un assistant IA personnel con√ßu pour vous aider dans votre vie quotidienne.\"},\n",
        "    {\"user\": \"Que sais-tu faire?\", \"assistant\": \"Je peux vous aider avec :\\n- Productivit√© (emails, calendrier, t√¢ches)\\n- Finance (crypto, actions, devises)\\n- Divertissement (films, musique, jeux)\\n- Programmation (code, debug)\\n- Utilitaires (m√©t√©o, traduction, calculs)\"},\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Dataset cr√©√© : {len(TRAINING_DATA)} exemples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir en format Dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_conversation(item):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": item[\"user\"]},\n",
        "            {\"role\": \"assistant\", \"content\": item[\"assistant\"]},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "formatted_data = [format_conversation(item) for item in TRAINING_DATA]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "print(f\"‚úÖ Dataset format√© : {len(dataset)} exemples\")\n",
        "print(f\"\\nExemple :\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Chargement du Mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Configuration 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"üì• Chargement du tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"üì• Chargement du mod√®le (4-bit)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"‚úÖ Mod√®le charg√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appliquer LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Fonction de formatage\n",
        "def formatting_func(example):\n",
        "    return tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
        "\n",
        "# Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",  # D√©sactiver wandb par d√©faut\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    formatting_func=formatting_func,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configur√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ LANCER L'ENTRA√éNEMENT\n",
        "print(\"=\"*60)\n",
        "print(\"  üöÄ D√âBUT DU FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  ‚úÖ FINE-TUNING TERMIN√â!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Sauvegarde du Mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarder le mod√®le LoRA\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Mod√®le LoRA sauvegard√© dans : {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fusionner les poids LoRA\n",
        "MERGED_DIR = f\"{OUTPUT_DIR}-merged\"\n",
        "\n",
        "print(\"üîÑ Fusion des poids LoRA...\")\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "print(f\"‚úÖ Mod√®le fusionn√© sauvegard√© dans : {MERGED_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Test du Mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "def chat(user_message, max_tokens=256):\n",
        "    \"\"\"Envoie un message au mod√®le et affiche la r√©ponse.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"\\nüë§ USER: {user_message}\")\n",
        "    print(\"ü§ñ A.B.E.L: \", end=\"\")\n",
        "    \n",
        "    model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tests\n",
        "chat(\"Bonjour A.B.E.L!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat(\"Quel temps fait-il √† Paris?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat(\"√âcris une fonction Python pour calculer le factoriel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat(\"Cours du Bitcoin?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat(\"Qui es-tu et que sais-tu faire?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ T√©l√©charger le Mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compresser le mod√®le pour le t√©l√©chargement\n",
        "!zip -r /content/abel-qwen2.5-7b-merged.zip {MERGED_DIR}\n",
        "\n",
        "print(\"\\n‚úÖ Mod√®le compress√©!\")\n",
        "print(\"üì• T√©l√©chargez le fichier depuis le panneau de fichiers √† gauche\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OU uploader sur Hugging Face Hub\n",
        "# D√©commentez et configurez si vous voulez upload sur HF\n",
        "\n",
        "# from huggingface_hub import login, HfApi\n",
        "# login(token=\"YOUR_HF_TOKEN\")\n",
        "# \n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=MERGED_DIR,\n",
        "#     repo_id=\"YOUR_USERNAME/abel-qwen2.5-7b\",\n",
        "#     repo_type=\"model\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Termin√©!\n",
        "\n",
        "Votre mod√®le A.B.E.L fine-tun√© est pr√™t!\n",
        "\n",
        "**Options:**\n",
        "1. T√©l√©charger le zip depuis le panneau de fichiers\n",
        "2. Upload sur Hugging Face Hub\n",
        "3. Sauvegarder sur Google Drive\n",
        "\n",
        "**Utilisation locale:**\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./abel-qwen2.5-7b-merged\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./abel-qwen2.5-7b-merged\")\n",
        "```"
      ]
    }
  ]
}
